---
title: "Predicting Waterpoint Functionality in Tanzania"
author: "Kyle Karber"
date: "May 17, 2019"
output:
  html_document: default
  word_document: default
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      error = FALSE,
                      fig.width = 5,
                      # fig.height = 3.5,  # set width only, not fixed aspect ratio 
                      fig.align = "center")
library(tidyverse)
library(cowplot)
library(gridExtra)
# library(ggpubr)
library(caret)
library(randomForest)
library(lubridate)
# library(googleVis)
library(lsr)
library(corrplot)
library(ggmap)
library(kableExtra)
library(png)

options(scipen=10000)

status_colors <- c("#0072B2", "#E69F00", "#CF3816")
cb_colors <- c("#999999", "#E69F00", "#56B4E9", "#009E73", 
               "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

```{r data, include=F}
getwd()
## import train and test data
train <- read.csv(file="../data/train.csv", header=T)
# need test data?

## load basemap
base_map <- readRDS("base_map.rds")

## models and results
model_rfe <- readRDS("../models/feature_elimination.rds")
model_rf_final <- readRDS("../models/rf_final.rds")
results_rf_tests <- readRDS("../models/results_rf_tests.rds")
results_rf_trees <- readRDS("../models/results_rf_trees.rds")
model_rf_mtry <- readRDS("../models/rf_mtry.rds")

## change certian names for plot aesthetics
# status_group 
levels(train$status_group)[levels(train$status_group)=="functional needs repair"] <- "needs repair"
levels(train$status_group)[levels(train$status_group)=="non functional"] <- "non-functional"
# source
levels(train$source)[levels(train$source)=="rainwater harvesting"] <- "rainwater"
# waterpoint_type_group
levels(train$waterpoint_type)[levels(train$waterpoint_type)=="communal standpipe multiple"] <- "multiple standpipes"
# rf tests names
results_rf_tests <- results_rf_tests %>% mutate(test_name = str_replace(test_name, "rf_", ""))


```
## Executive Summary


## Introduction
#### Background


#### describes the dataset
* wells and the associated pumps for drinking water, which I will predominantly refer to as waterpoints. 


* class imbalance

```{r status_group}

train %>% ggplot(aes(status_group)) +
  geom_bar(aes(y=(..count..)/sum(..count..), fill=status_group), width=0.7) +
  # ggtitle("Waterpoint Condition") +
  geom_text(stat='count', aes(y=(..count..)/sum(..count..), 
                              label=paste("n = ", ..count..)), vjust=-0.2) +
  xlab("") +
  ylab("proportion") +
  scale_x_discrete(breaks=levels(train$status_group), 
                       labels=c("functional", 
                                "functional,\nneeds repair",
                                "non-functional")) +
  scale_y_continuous(limits=c(0, 0.6)) +
  scale_fill_manual(values=status_colors) +
  theme(legend.position="none")
```

#### summarizes the goal of the project 
* The objective of this project is to ...
... competition performance is secondary to having a useful tool for predicting waterpoint failure not only at the time this data was collected but also predict likely failures in the future


#### summarizes key steps that were performed


## Methods

*explains the process and techniques used, such as data cleaning, data exploration and visualization, 
*any insights gained 
*modeling approach

#### Data Exploration and Preprocessing

I began by exploring the data to get a better understanding of the various variables and their potential usefulness for predicting the condition/functionality of waterpoints in Tanzania. I made an initial pass through each of the variables looking at summary statistics, plotting distributions for the numeric features, and looking at summary tables for categorical features to get a better understanding of their meaning and potential usefulness in predicting waterpoint status; I removed features with little to no variance and redundant features. Through exploration I also identified features that required log transformations to normalize the data, features that required imputation, instances of data leakage, and other potential issues with the data. 

During the initial pass I did not consider the features against the target variable (e.g. I did not plot features broken down by status group), so that I did not bias the feature selection process; feature selection should occur within the resampling (cross-validation) procedure, not using the entire training set (APM p.500, EoSL p.246-247). There are more details on the resampling procedure and feature selection process in the following sections. After I down-selected the features to include in the modeling process using the methods outlined above and below, I explored the variables further, including visualizing their relationship with the target variable.

There were a significant number of hierarchical features with overlapping levels, making them highly correlated.  I will refer to these as grouped features. For example, `source`, `source_type`, and `source_class` are all directly related to the water source, but they range in the amount of detail in the levels - they have 10, 7, and 3 unique levels, respectively. I did not want to include all of the features within a group in the model and feature selection process due to issues arising from their correlation, such as multicollinearity in the case of logistic regression. I initially selected one variable from each group based on the number of unique levels (not too many or too few) and the number of waterpoints in each level – if a certain feature had levels with less than 100 datapoints and that level was combined with another level in a different feature within the group, the less detailed feature was chosen. 

While some of the features were obviously correlated due to their hierarchical nature, other features needed to be tested for correlation. Since the features included a mix of numeric and categorical data, I used a custom function to calculate correlations. This function used Pearson's correlation for numeric pairs, chi-squared test for categorical pairs, and the linear regression correlation coefficient for numeric-categorical pairs. Features with correlation coefficients greater than |0.5| were not included in the model building process at the same time. Fortunately, there were very few feature pairs that exceeded this threshold. 

#### Model Evaluation

Before discussing my modeling methods, I will explain the methods I used to measure model performance. The primary metric I used to evaluate model performance was the overall accuracy, mostly because this is the only metric provided by drivendata.org when submitting predictions for the unlabeled test set. As such, I used this metric during the model building process. However, this was not the only metric I considered. Since the consequences for people lacking access to clean drinking water is much higher than the consequences of sending a repair team to a functioning well, sensitivity to predicting non-functional wells is a high priority, so I also considered this metric and the ROC curves. 

To choose between different models and variations thereof, I needed an accurate method of assessing their accuracy. If I only measured accuracy based on how well the models perform on the data used to fit the model, and selected and tuned the models accordingly, I would have ended up with an overly complex model that was overfit and would have much lower accuracy when making predictions on new data. In order to avoid this, I needed to fit the models to a portion of the data, a training set, and then measure their performance on a different portion of the data, a validation set. However, researchers have shown that judging model performance on a single test set is less than ideal (APM p.67). 

An alternative to using a single validation set is to resample the dataset, creating several different train and validation sets, then using statistics to get a realistic estimate of model performance (APM p.66). I employed a resampling technique that is common used to estimate the accuracy of predictions – k-fold cross-validation (EoSL p.241). Usually, k is 5 or 10, with a larger value of k decreasing the bias of the estimate (APM p.70), and I chose to use k=10 for model assessment. There are other resampling techniques, such as bootstrap methods and leave-one-out cross-validation (k-fold where k=n), but for relatively large sample sizes 10-fold cross-validation should have reasonably low bias, acceptable variance, and faster computation time compared to these other techniques (APM p.78).

In 10-fold cross-validation the dataset is divided into 10 equal parts, and the model is fit 10 times. In each iteration the model is fit to 9 parts of the data and the accuracy or error of the fit is measured on the 10^th^ part (validation set). There are 10 iterations, so that each of the 10 parts is used as the validation set. For a more detailed explanation of k-fold cross validation see (EoSL pp.241-249) or (APM pp.69-71). 

10-fold cross validation should give a reasonable approximation of model performance on new unseen data. To test the real-world generalizability of my final models, I used them to predict the waterpoint status on an unlabeled test set (i.e. waterpoint status unknown). I submitted these predictions to drivendata.org which returned a final accuracy score. I did not use the test set for model selection or model tuning, only for the final performance evaluation. 

#### Model Selection

In order to select the best model for predicting waterpoint status, I followed the model selection guideline provided by (APM p.79):
1. Start with several models that are the most flexible (but least interpretable) -- models that have a high likelihood of being the most accurate across a variety of domains. These models establish the "performance ceiling." I chose to use a random forest classifier and gradient boosted decision tree classifier (xgboost) to establish the performance ceiling.
2. Apply simpler more interpretable models for comparison. I chose to use multinomial logistic regression and k-nearest neighbors models for this step. 
3. Use the simplest model that closely approximates the performance of more complex models. 

When comparing model performance I selected the most parsimonious (i.e. simplest) model that was within one standard error of the model with the best accuracy, which is a common rule of thumb (EoSL p.244). I provide a brief overview of the candidate models below. 

**Random Forest**  
A random forest classifier is the average of a “forest” of decision tree classifiers. Decision tree classifiers are essentially made up of nested `if-then` statements that aim to partition the data into smaller groups with a larger proportion of one class after each `if-then` statement. These classifiers are simple, interpretable, and can handle categorical and missing data, but they are very noisy and unstable (APM p.369). Bagging (bootstrap aggregation) reduces the variance of decision trees by averaging many of them into an ensemble or “forest”(EoSL p.588)

The random forest further improves on the bagged trees by introducing randomness: a limited number of predictors/features are randomly sampled at each split/node in the tree. By introducing randomness, the trees are de-correlated and variance of the estimate is further reduced. Random forest is popular because it performs well, requires very little tuning, is robust to noisy features, provides estimates of variable importance, can handle missing data, and it rarely overfits the data. For more information on random forests, see (EoSL pp.587-602 and APM pp.198-203). 

**XGBoost**  
Boosting is a relatively new technique whose theory and algorithms were developed in the 1980’s and 1990’s, respectively (APM p.204). Boosting algorithms take many weak classifiers whose error rate is only slightly better than random guessing and combines them into a strong classifier (EoSL p.337). Decision trees are a good base learner for boosting because they can be made weak learners by restricting their depth and they are easily ensembled together. Thus, gradient boosted trees have some basic similarities with random forests, but they have significant differences: in random forests the trees are independent, deep, and equally weighted in the ensemble, whereas with gradient boosting the trees are dependent on past trees, shallow, and unequally weighted in the ensemble. Gradient boosted trees and random forests offer similar predictive performance, but random forests are easier to tune and can be computed faster since parallel processing is straightforward due to the independent trees (APM pp.205-206). For more information on boosting, see (APM pp.203-208 & pp.389-392).

XGBoost is a newer variation on the gradient boosting technique and was originally released in 2014. XGBoost quickly gained popularity due to its winning performance in Kaggle machine learning competitions; of the 29 Kaggle challenge winning solutions in 2015, 17 solutions used XGBoost while the next most popular method was deep neural nets which were used in 11 solutions. This method is applicable to a wide range of problems, has a faster computation time than deep learning methods with similar performance in many instances. For more information on XGBoost, see (XGBoost).

**Multinomial Logistic Regression**  
Logistic regression is a simple and interpretable classification technique, but it is unlikely to perform as well as the previous two models. It is a variation on linear regression where the logit function, $log(p/(1-p))$, is used to convert the continuous outcome variable to a probability distribution between 0 and 1 that is applied to a binary outcome variable. Multinomial logistic regression extends logistic regression to outcome variables with three or more classes. For more information on logistic regression, see (APM p.282-287 and EoSL p.119-128)

**k-Nearest Neighbors**  
The k-nearest neighbors (KNN) predicts the class of a sample based on the majority vote of the classes of the k-nearest datapoints. The nearest datapoints are determined by the Euclidian distance or another distance metric. Thus, features must be numeric or categorical that are able to be converted to numeric, and they must be centered and scaled if the units of any of the features are different. KNN is simple and has good performance on certain datasets (e.g. data with irregular decision boundary), but it is not interpretable for high-dimensional data, prone to overfitting with smaller k, can be unstable, and it does not take categorical features. For more information on KNN, see (EoSL p.463-468, APM p.350-353).

**Ensemble Model**  
I created a simple majority vote ensemble model that combines all four models. In the event of a tie the prediction of the top performing model is selected, so only a unified dissenting vote can overrule the top model. 

**Final Models**  
These are the final models that I used to predict the target variable on the test set for submission to drivendata.org: the best single model above and the ensemble model, and also the best single model with the leaky predictors included for comparison purposes. 

#### Feature Selection and Parameter Tuning
I reduced the number of potential features using the methods outlined in the Data Exploration section; I removed features that were redundant, had little to no variance, or that appeared to be data leakage and made an initial selection of a single variable among those belonging to a group (though I later substituted the grouped variables for one another).

I worked to select the most relevant features in order to have a simpler more parsimonious model, even though Random forest and XGBoost essentially have feature selection built in, in that unimportant predictors are (usually) not included in the tree building process. To select the most important features I used recursive feature elimination, using the `rfe()` function in the `caret` package, which employs the algorithm in the figure below.  

![](../images/rfe_algo.png)
(https://topepo.github.io/caret/recursive-feature-elimination.html)

For each of the features that belonged to a group and remained after the recursive feature selection process, I substituted the other variables in within the feature group one-by-one and reevaluated the model performance. Of the grouped features, I selected the simplest feature that didn't negatively impact model accuracy.

It is important to tune the model parameters in order to maximize performance on the validation set and not overfit to the train set. In order to tune model parameters I used the `train()` function from the `caret` package. This function repeatedly fits the model using a specified or auto-generated grid of tuning parameters. For the random forest model (using ‘method=rf’) the only tuning parameter was `mtry`, which is the number of variables to randomly sample at each split. The only tuning parameter for the KNN model was `k`, the number of neighbors. The only tuning parameter for the multinomial logistic regression was `decay`, the weight decay for the penalty parameter. XGBoost has seven tuning parameters, `nrounds` (# boosting iterations), `max_depth` (max tree depth), `eta` (shrinkage), `gamma` (minimum loss reduction), `colsample_bytree` (subsample ratio of columns), `min_child_weight` (minimum sum of instance weight), `subsample` (subsample percentage) (https://topepo.github.io/caret/available-models.html). 

`ntrees` is not considered a model performance parameter because performance will quickly plateau with a sufficient number of trees (e.g. >200), but more trees will not improve the accuracy, nor will they cause overfitting until an excessive amount of trees are used (e.g. >2000). However, I tuned the number of trees in order to use the minimum number of trees to get to the performance plateau with the lowest computation time. 

Though not parameter tuning, I did an additional test of performance on the random forest model. Tree models are capable of directly taking categorical predictors, while most models require categorical predictors to be decomposed into a series of binary dummy variables (or other encoding methods) to represent the different levels/categories (APM p.372-373). I tested the performance of the random forest model using the categorical predictors as-is and using them as dummy variables. When fitting a model using the formula call (i.e. `train(form, data, …)`) categorical predictors are automatically coded as dummy variables, whereas they are used as-is when using the “x,y” call (i.e. `train(x, y, …)`) (https://github.com/topepo/caret/issues/370)


## Exploratory Data Analysis

The dataset consists of 40 features (predictor variables), including the waterpoint name and ID. The target (outcome variable) of interest is the waterpoint condition (`status_group`). The majority of the data was recorded between 2011 and 2013, but some of the data was recorded as early as 2002. I've organized the features into five descriptive categories: location features, funder and installer, waterpoint features, payments and management, and unused features. I explore the features in these categories in more detail below. 

There were a number of features that were either redundant or their meaning was unclear, so they were not included in the modeling process. This includes: `wpt_name` (redundant with `id`), `num_private` (unknown meaning, low variance), `public_meeting` (unknown meaning, low variance), `recorded_by` (zero variance), and `scheme_name` (unknown meaning, likely redundant).


#### Location Features

There were eight features directly related to the waterpoint location (latitude, longitude, subvillage, ward, lga, region, region_code, and district code) and three features related to location (population, altitude, basin). Subvillage, ward, lga, region, and district are all geographic subdivisions, from smallest to largest. Although region is a subdivision of district, there are more unique regions codes (27) than district codes (20). Additionally, the number of unique region names (21) is smaller than the number of regions codes. Because of these discrepancies, I decided to focus on the latitude and longitude location features. About 3% of the longitude data is missing (zeros), but none of the latitude data is missing. The latitude longitude coordinates of the waterpoints colored by functionality are shown in the figure below. 

```{r map, fig.width=8.5, message=F}
# load basemap
base_map <- readRDS("base_map.rds")
# plot waterpoints on base map
ggmap(base_map) +
  geom_point(aes(x=longitude, y=latitude, color = status_group), data=train, 
             shape=16, alpha=0.35) +
  scale_color_manual(values=status_colors) +
  theme_void() +
  theme(legend.title = element_blank()) +
  guides(colour = guide_legend(override.aes = list(size=3, alpha = 0.8)))

```

Nearly half of the population data was missing (recorded as zeros or ones), so I tried two different imputation methods. The first simply imputed the median population of the region (`region_code`) where the waterpoint is located. However, because of the large amount of missing population data, about one-fourth of the regions had a median population of zero. Thus, even after imputation, about one-third of the population data was still zero. 

The second method used the k-nearest neighbors (KNN) algorithm, with latitude and longitude as the only inputs and the log10 transform of population as the target. Using 10-fold cross-validation, the R^2^ was 0.58 and the RMSE was 0.31. It is not straightforward to interpret the RMSE of a log transformed variable; it is not simply $10^{0.31}=2.0$. For some context, the mean population is 15 and if you were to add and subtract the RMSE from this value you would get 31 and 8 people, respectively. While the imputation error is likely better than that of imputing the overall median or medain by region, there is still significant error. In the future I will include publicly available population density data in the imputation to further improve accuracy. 

Since missing numerical data is recorded as zeros in this dataset, we cannot differentiate between missing `gps_height` data and waterpoints that are actually at sea level. In the future, I will verify the accuracy of the gps heights by checking it against elevation data for a given latttude and longitude. The `basin` feature is comprised of the geographic water basins.

* included lga as the location variable for the logistic regression instead of latitude and longitude, since it is very unlikely that source would be linearly dependent on GPS coordinates. 

#### Funder and Installer 
The dataset includes information on who funded the well (`funder`) and who installed it (`installer`). There were 1898 unique funders and 2146 unique installers recorded. However, they were not truely unique as the same funder/installer was sometimes recorded under different names. For example, "Government", "Gover", "GOVER", "Govt", "Central government." I first converted all levels to lowercase strings, then identified the government related levels with the regex `"(gov)|(dist)|(coun)"`. I then examined the strings with the detected pattern and developed a regex to exclude misidentified strings, `"(village)|(comm)|(china)|(belgian)|(finland)|(irish)|(ital)|(japan)|(iran)|(egypt)|(methodist)"`. I similarly identified community/village funders and installers with the regex `"(comm)|(vill)"`, while excluding `"(committe)|(bank)"`. I retained the top 10 and 20 funders and installers, and changed the rest to "other." There is significant overlap between the top funders and installers. The figure below shows the top 10 installers broken down by waterpoint condition. 

```{r installer, fig.height=4}

train %>% group_by(installer10) %>% 
  mutate(installer_tot = n()) %>% ungroup() %>%
  ggplot(aes(x=reorder(installer10, status_group=="non-functional"), fill=status_group)) +
  geom_bar(position="fill") +
  geom_hline(yintercept=0.384, size=1) +
  geom_text(aes(label=paste("n = ", installer_tot), y=1), hjust=1) +
  coord_flip() +
  scale_fill_manual(values=status_colors) +
  ylab("proportion") +
  xlab("installer") +
  theme(legend.position=c(-0.3, 1.08), 
        legend.title = element_blank(), 
        legend.spacing.x=unit(2, "mm"),
        legend.direction="horizontal")
# could add tick at y-intercept=0.38
```

```{r funders, include=F}

funder_plot <- train %>% group_by(funder10) %>% 
  mutate(funder_tot = n()) %>% ungroup() %>%
  ggplot(aes(x=reorder(funder10, status_group=="non-functional"), fill=status_group)) +
  geom_bar(position="fill") +
  geom_hline(yintercept=0.384) +
  geom_text(aes(label=funder_tot, y=1), hjust=1) +
  coord_flip() +
  scale_fill_manual(values=status_colors) +
  ylab("proportion") +
  xlab("funder") +
  theme(legend.position="top", legend.title = element_blank())

# grid.arrange(funder_plot, installer_plot, nrow=1)
# how to combine legend?
# ggarrange(funder_plot, installer_plot, nrow=1, common.legend=TRUE, legend="top")
```


#### Waterpoint Features

There are a number of waterpoint features in this dataset, including the construction year, permit, water source, extraction type, waterpoint type, and water quality. A little over one-third of the construction year data was missing. In the furure I will impute missing construction year data. The figure below shows the construction year data broken down by functionality. 

```{r construction_year, fig.width=7}
# histogram for construction_year grouped by status_group
train %>% filter(construction_year > 0) %>%
  ggplot(aes(construction_year, fill=status_group)) + 
  geom_histogram(bins = 15, color="grey40") + 
  scale_fill_manual(values=status_colors) +
  xlab("construction year") +
  facet_grid( ~ status_group) +
  theme(legend.position = "none")
```
I created a new variable, `years_op` (i.e. years operational), that was the time elapsed between the construction year and the year this data was recorded, since it was recorded over an 11 year timeframe. This was intended to help narrow down the number of years that the waterpoint was operational, but since we do not have data on the actual year of failure this is a source of uncertainty. This new variable made it possible to predict which wells have failed since the data was recorded, by adding the number of years elapsed since the data was recorded to `years_op` and re-running the model. In addition to giving updated predictions on waterpoint failures, tweaking this variable and re-running the model will validate the stability and usefullness of the model: I expect that the number of non-functional waterpoints will increase and waterpoints previously predicted to be non-functional should not change to functional when `years_op` is increased. 

```{r years_op?}
# figure with years_op histogram? 
```

The next waterpoint feature is `permit`. While not explicitly defined, this feature most likely represents whether a permit was recieved for the installation of a waterpoint. The table below shows permit broken down by the functional status of the well. Functional status does not vary much by permit, so this feature is likely to have little predictive power. However, it was still included in the recursive feature elimination variable selection process.

```{r permit}
prop.table(table(train$permit, train$status_group), margin = 1) %>% 
  kable(digits = 2) %>% kable_styling(bootstrap_options = "condensed", full_width = F)
```

The next waterpoint feature group is the water source, which includes `source`, `source_type`, and `source_class`, with 10, 7, and 3 unique levels respectively. My inital selection was `source_type`, but the more detailed `source` variable is broken down by functional status in the figure below.

```{r source, fig.height=4}
train %>% group_by(source) %>% 
  mutate(tot = n()) %>% ungroup() %>%
  ggplot(aes(x=reorder(source, status_group=="non-functional"),
             fill=status_group)) +
  geom_bar(position="fill") +
  geom_hline(yintercept=0.384, size=1) +
  geom_text(aes(label=paste("n = ", tot), y=1), hjust=1) +
  coord_flip() +
  scale_fill_manual(values=status_colors) +
  ylab("proportion") +
  xlab("water source") +
  ggtitle("") + # needed for blank space for legend position
  theme(legend.title=element_blank(),
        legend.position=c(-0.3, 1.08),
        legend.spacing.x=unit(2, "mm"),
        legend.direction="horizontal")
```

The next waterpoint feature group was the extraction type, meaning the method of extracting water from the water source. This group included `extraction_type`, `extraction_type_class`, and `extraction_type_group`, with 18, 13, and 7 unique levels, respectively. My inital selection was `extraction_type_class`, which is broken down by functional status in the figure below.

```{r extraction_type, fig.height=3}
train %>% group_by(extraction_type_class) %>% 
  mutate(tot = n()) %>% ungroup() %>%
  ggplot(aes(x=reorder(extraction_type_class, status_group=="non-functional"),
             fill=status_group)) +
  geom_bar(position="fill") +
  geom_hline(yintercept=0.384, size=1) +
  geom_text(aes(label=paste("n = ", tot), y=1), hjust=1) +
  coord_flip() +
  scale_fill_manual(values=status_colors) +
  ylab("proportion") +
  xlab("extraction  type") +
  ggtitle("") + # needed for blank space for legend position
  theme(legend.title=element_blank(),
        legend.position=c(-0.3, 1.08),
        legend.spacing.x=unit(2, "mm"),
        legend.direction="horizontal")
```

The next waterpoint feature group was the waterpoint type, meaning the method of dispensing the water. This group included `waterpoint_type` and `waterpoint_type_group`. The only difference was that the latter combined "community standpipe" with "community standpipe multiple." The `waterpoint_type` with the additional bit of detail was the feature that I initally selected in this group, and it is shown by status group in the figure below.

```{r waterpoint_type, fig.height=3}
train %>% group_by(waterpoint_type) %>% 
  mutate(wp_type_tot = n()) %>% ungroup() %>%
  ggplot(aes(x=reorder(waterpoint_type, status_group=="non-functional"),
             fill=status_group)) +
  geom_bar(position="fill") +
  geom_hline(yintercept=0.384, size=1) +
  geom_text(aes(label=paste("n = ", wp_type_tot), y=1), hjust=1) +
  coord_flip() +
  scale_fill_manual(values=status_colors) +
  ylab("proportion") +
  xlab("waterpoint  type") +
  ggtitle("") + # needed for blank space for legend position
  theme(legend.title=element_blank(),
        legend.position = c(-0.4, 1.08),
        legend.spacing.x=unit(2, "mm"),
        legend.direction = "horizontal")
```

Additionally, two feature groups were flagged as data leakage, meaning they provide information that would not realistically be available at the time of prediction. The first "leaky" feature group is `quantity` and `quantity_group`. These features qualitatively reveal how much water flows from the waterpoint. In order to obtain quantity data we would also observe the functional status of the well. For example, waterpoints with quantity of "dry" are almost entirely non-functional. Presumably, when making new predictions in the future quantity data would not be available for waterpoints with an unknown functional status. As such, I have developed my models without the quantity features. For comparison purposes, I added `quantity` to my final models to see how much this leaky predictor improves model accuracy. 

The second leaky feature group is water quality, including `water_quality` and `quality_group`. One of the levels of this feature is "unknown" and waterpoints in this category are predominantly non-functional. Water quality could not be assessed without a functioning well (except when the source is a river, lake, etc.), thus unknown water quality leaks information about the functionality of the well. 

The majority of waterpoints have "good" water quality (50,818), 5,195 have "salty" water, 1,876 have unknown water quality, and the remaining 1,511 have either milky, colored, or fluoride water quality.


#### Payments and Management
There are two related payment features in the dataset: payment amount in Tanzanian Shillings (`amount_tsh`) and the type or frequency of payment (`payment_type`), which could be by the bucket, monthly, or yearly for example. At first glance it may appear that 70% of the `amount_tsh` data is missing (recorded as zeros) but when you account for zeros in the `payment_type` categories of "never pay," "unknown," and "on failure," less than 10% of the data appear to be erroniously recorded as zero. A smoothed histogram of non-zero payment amounts in Tanzanian Shillings (Tsh) broken down by payment type is shown below. Note that one US dollar was equivalent to about 1,500 Tsh around the time most of these data were recorded. 

```{r amount_tsh, include=F, fig.width=7}
train %>% filter(amount_tsh > 0, !payment_type=="never pay", 
                 !payment_type=="other", !payment_type=="unknown") %>% 
  ggplot(aes(amount_tsh, fill=payment_type)) +
  geom_histogram(bins=30) +
  scale_x_log10(limits=c(1,350000)) +
  #theme_bw() +
  scale_fill_manual(values=cb_colors, name="payment type") +
  xlab("payment amount in Tsh (log scale)")
```

```{r amount_tsh2, fig.width=7, message=F}
train %>% filter(amount_tsh > 0, !payment_type=="never pay", 
                 !payment_type=="other", !payment_type=="unknown") %>% 
  ggplot() +
  geom_density(aes(x=amount_tsh, y=(..count..)*2, fill=payment_type),  
               adjust=2, alpha=0.5) + # multiply count by adjust value to get correct count
  scale_x_log10(limits=c(1,350000)) +
  #theme_bw() +
  scale_fill_manual(values=c("darkturquoise", "red", "orange", "darkgrey"), name="payment type") +
  xlab("payment amount in Tsh (log scale)") +
  ylab("count")
```

The median payment per bucket was 20 Tsh (~\$0.01), the median monthly payment was 300 Tsh (~\$0.20), and the median annual payment was 2000 Tsh (~\$1.33). Thus, the relationship between payment amount and payment type is as one might expect. The log<sub>10</sub> transform of non-zero values of `amount_tsh` were saved as a new feature (`amount_log`) because its distribution was closer to normal (zeros were retained). 

Note that the limited information provided on this dataset shows `amount_tsh` to be the "total static head," which is essentially a measure of water pressure. If this was the true meaning, this variable should be highly correlated with the water quantity as well as the waterpoint status. I.e. if there is zero water pressure there would be zero water flow, so `quantity` should be dry and `status` should be "non-functional." However, the relationship between `amount_tsh` and these two variables is not as expected. The correlation coefficents between log<sub>10</sub>(`amount_tsh`) and `quantity` and `status_group` are 0.18 and 0.21, respectively. The correlation coefficents between log<sub>10</sub>(`amount_tsh`) and `payment_type` is 0.78. Additionally, total static head is typically measured in feet or meters, and many of the values for `amount_tsh` are astronomical compared to even high-horsepower waterpumps, nevermind handpumps. 

There are three management related features which are assumed to represents who manages the well, including the payment process -- though the exact meaning of these variables is unclear. These features are `scheme_management`, `management`, and `management_group`, with 10, 9, and 5 unique levels respectively. I selected `management` as the inital feature for model building.


#### Correlations


#### Summary of Features


## Results


#### Random Forest

**Recursive Feature Elimination**  
The results of the recursive feature elimination are shown in the figure below, with the error bars representing the 95% confidence interval in this and all other plots. There is a significant increase in accuracy as the number of variables included in the random forest model increases from 4 to 10. Though the 12 variable model has the highest accuracy, its accuracy is not significantly better than that of the 10 variable model. As such, I chose to use the 10 variable random forest model going forward.
```{r rf_rfe}

model_rfe$results %>% ggplot(aes(Variables, Accuracy)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin=Accuracy-1.96*AccuracySD/sqrt(10), 
                    ymax=Accuracy+1.96*AccuracySD/sqrt(10)), width=0.5) +
  scale_x_continuous(name="Number of Variables", breaks=c(2:6)*2)
  
```

The variable importance for the 12 variable model is shown in the figure below. `permit` and `source_type` were removed for the 10 variable random forest model.

```{r rf_imp, fig.width=6.5}

varImpPlot(model_rfe$fit, main=NULL)
```
**Parameter Tuning**  
The RFE results were obtained with `mtry=3`, which is the number of variables randomly sampled at each node, and with `ntrees=500`. I used a search grid of `mtry=c(2,3,4,5)`. For 2 and 5 accuracy was slightly reduced, but was essentially the same for 3 and 4, and not significantly different for any value of `mtry`, so I kept it at 3. I used values of `ntrees=c(51,101,201,301,501). Similarly, There was not a significant difference for any value of `ntrees`, but there was a slight upward trend. I chose `ntrees=201` even though it had lower accuracy compared to 101 or 301, as that dip is likely due to variance, and 201 should provide an adequate number of trees with a faster compute time than larger forests. Overall, you can see that these parameters have very little effect on the accuracy.

```{r rf_mtry_trees, fig.width=6.5}

p_mtry <- model_rf_mtry$results %>% ggplot(aes(mtry, Accuracy)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin=Accuracy-1.96*AccuracySD/sqrt(10), 
                    ymax=Accuracy+1.96*AccuracySD/sqrt(10)), width=0.5) +
  scale_y_continuous(limits=c(0.7635, 0.7735)) +
  ylab("accuracy")

p_trees <- results_rf_trees %>% ggplot(aes(n_trees, accuracy)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin=accuracy-ci, ymax=accuracy+ci)) +
  scale_y_continuous(limits=c(0.7635, 0.7735)) +
  ylab(NULL)

grid.arrange(p_mtry, p_trees, nrow = 1)

```

**Variable Substitutions**  
After establishing the 10-variable baseline, I substituted grouped variables for one another to see if further improvements in accuracy were possible. The figure below compares the accuracy of these variable substitutions.

```{r rf_tests}
# only needed for testing
# results_rf_tests <- results_rf_tests %>% mutate(test_name = str_replace(test_name, "rf_", ""))

results_rf_tests %>% filter(!test_name %in% c("leak", "final")) %>%
  ggplot(aes(test_name, accuracy)) +
  geom_point() +
  geom_errorbar(aes(ymin=accuracy-ci, ymax=accuracy+ci), width=0.5) +
  xlab("features substituted") 
  
```

For the "ext" substitution, I replaced `extraction_type_group` with `extraction_type_class`, which resulted in a nearly significant decrease in accuracy. For "fund" I added `funder20`, which slightly improved accuracy. For the "inst" substitution, I replaced `install20` with `install10`, which did not affect accuracy. For the "man" substitution, I replaced `management` with `management_group` or `scheme_management`, neither of which affected accuracy. For the "pay" substitution, I replaced `payment_type` with `amount_log`, which significantly reduced accuracy. For the "pop" substitution, I replaced `pop_log3` with `pop_log`, which improved accuracy to an almost significant amount. For the "pop2" substitution, I replaced `pop_log3` with `pop_log2`, which also improved accuracy. For the "year" substitution, I replaced `years_op` for `construction_year`, which had a negligible increase in accuracy. 

Though none of the substitutions resulted in a significant improvement in accuracy, adding `funder20` and substituting `pop_log` for `pop_log3` showed promise. I made both of these changes for the "final" model, which showed a significant improvement over the "baseline"model. This is shown in the data leakage figure below. 

**Data Leakage**  
The accuracy of the baseline, final, and leaky predictor models are compared in the figure below. Including the suspected leaky predictors, `quantity` and `water_quality`, significantly improves the model performance as expected. According to the mean decrease in the Gini coefficient, quantity becomes the most important variable in the leaky model while quality is the least important. This confirms that quantity is likely to be a leaky predictor, and quality may not be a leaky predictor but it also isn't a very useful predictor.
```{r rf_leak}
# only needed for testing
# results_rf_tests <- results_rf_tests %>% mutate(test_name = str_replace(test_name, "rf_", ""))

results_rf_tests %>% filter(test_name %in% c("baseline", "final", "leak")) %>%  #add final
  ggplot(aes(test_name, accuracy)) +
  geom_point(color="blue") +
  geom_errorbar(aes(ymin=accuracy-ci, ymax=accuracy+ci), width=0.2, color="darkgrey") +
  xlab("random forest models")
  
```
Maybe delete this plot ^ and refer to the plot below

#### All Model Comparison
I focused on the random forest model in terms of the analysis and presenting the results above because it had higher accuracy than the other models; I only provide a brief overview of the other model results in comparison to the random forest results and the final submission results in this section. In the figure below I show the accuracy for the random forest (rf), random forest with leaky predictors (leak), XGBoost (xgb), multinomial logistic regression (multinom), k-nearest neighbors (knn), majority vote ensemble (vote), and stacked ranfom forest ensemble (stack) models. The 10-fold cross validation accuracy estimates with 95% confidence intervals are represented by blue dots, while the test set accuracy of the final submissions are represented by red dots. 

```{r all_models, message=F}
results_all %>% ggplot() +
  geom_point(aes(model, accuracy), color="blue", alpha=0.8) +
  geom_errorbar(aes(x=model, ymin=accuracy-ci, ymax=accuracy+ci), width=0.3, color="grey") +
  geom_point(aes(model, test), color="red", alpha=0.8) +
  scale_x_discrete(limits=c("rf", "leak","xgb", "multinom", "knn", "vote", "stack"))

```
Of the individual models without leaky predictors, the random forest had the highest CV-estimated accuracy. In fact, the individual random forest model had better accuracy on the test set than either of the ensemble models, though the stacked random forest ensemble was within one ten-thousandth of the individual random forest, at 0.7800 and 0.7801 respectively. The random forest model with the leaky predictors had the best performance overall, at 0.8125 accuracy on the test set. This is within 2% of the best performing submission to the drivendata.org competition, which has an accuracy of 0.8286. The test set accuracy is within the confidence interval for the random forest and XGBoost models, and just above it for the model with leaky predictors. 

The XGBoost model was the next best individual model with a test set accuracy of 0.7601. This accuracy was acheived with tuning parameters of `nrounds=250, max_depth=5, eta=0.4, gamma=0, colsample_bytree=0.8, min_child_weight=1, subsample=0.75`, which had a CV-estimated accuracy of 0.758. The tuning grid had 500 different variations for these values (except `gamma` and `min_child_weight` which were held constant). The worst tuning parameters had a CV-estimated accuracy of 0.672, demonstarating that tuning parameters have a large impact on the performance of the XGBoost model and performance may be improved through additional tuning. In comparison, the random forest model is much more user frinedly because it only has one tuning parameter, `mtry`, which had little effect on the estimated accuracy in this case. 

Because of the consequences resulting from a lack of potable water, it is more important to identify wells that require repair than to identify (or misidentify) functioning wells. Thus, the sensitivity (a.k.a. recall or true positive rate) for predicting non-functional wells is an important model characteristic. The random forest model has a sensitivity of 0.72 for non-functional and 0.87 for functional, thus it is better at predicting functional wells, which is not ideal. The ROC curve for non-functional is shown in the figure below. 

```{r roc}
# set non-functional to 1 and functonal and functional needs repair to zero
y_roc <- model_rf_final$finalModel$y
levels(y_roc)[levels(y_roc)=="non functional"] <- 1
levels(y_roc)[levels(y_roc)=="functional"] <- 0
levels(y_roc)[levels(y_roc)=="functional needs repair"] <- 0
# probabilities for fnon-functional
x_roc <- model_rf_final$finalModel$votes[,3]
# calculate and plot ROC curve
roc_rf <- roc(y_roc, x_roc)
ggroc(roc_rf, color="blue") +
  geom_point(aes(x=0.8910, y=0.7217), color="red")  # sensitivity/specificity values from confusion matrix

```
Similarly, the XGBoost model has a sensitivity of 0.69 for non-functional and 0.87 for functional, so it is slightly worse at predicting non-functional wells. In the future I could change the prediction cutoff values to a value that increases sensitivity while maintaining an acceptable number of false positives. It is also worth noting that neither the random forest nor the XGBoost models predicted "functional, needs repair" waterpoints with a sensitivity above 0.30, due to the significant class imbalance. In the future I can take steps to address the class imbalance or simply combine this class with one of the others. 


#### Updated Prediction 
After adding the number of years between `date_recorded` and 2019 to `years_op`, it increased from an average of 14 years to 21 years. Predicting waterpoint status using the updated `years_op` in the final random forest model resulted in 4667 prediction class changes. 1986 waterpoints changed from functional to non-functional, 473 changed from needs repair to non-functional, and 51 changed from functional to needs repair. These changes reflect the expected behavior--older wells are more likely to fail--but not all the changes follow this behavior. 1374 waterpoints changed from non-functional to functional, 757 changed from needs repair to functional, and 26 changed from non-functional to needs repair. 

Unfortunately, we do not have a way of verifying the accuracy of the updated predictions; while it is logical that older wells are more likely to fail according to some monotonically decreasing function, the model is likely to capture greater complexities and nuances. For example, a certain installer may have changed their methods or materials over the years, causing reliability and longevity to fluctuate over time. A linear/logistic model might better represent our expectation that waterpoints are more likely to be non-functional with increasing age, but it would not capture such nuances. Additionally, if my example is true, it would be better to use the `construction_year` feature instead of `years_op`. 


## Conclusion



* I attempted to identify additional wells that are likely to have failed since the data was originally collected, but waterpoint status predictions changed from non-functional/needs repair to functional almost as much as they changed from functional to non-functional/needs repair; this unexpected behavior calls into question the reliability of these new predictions, and the generalizability of the model over time. Thus, additional work is needed to validate the updated predictions and/or develop a model that better accounts for changes over time.

* In the future, I would like to address the "needs repair" class imbalance. I would also like to use the dataset to optimize resource allocation with regard to repairing waterpoints, e.g. target areas with large numbers of non-functional wells, incorporate the predicted probabilities not just the predicted class to target well that are most like to be non-functional, etc.




